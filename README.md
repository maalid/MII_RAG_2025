# ğŸ“š RAG Indexing y Consulta con LlamaIndex + Azure/OpenAI

Este proyecto permite crear y consultar Ã­ndices vectoriales persistentes sobre documentos usando [LlamaIndex](https://github.com/jerryjliu/llama_index), integrando modelos de embedding y respuesta tanto de OpenAI como de Azure OpenAI.

Se utiliza un enfoque RAG (Retrieval-Augmented Generation) para responder preguntas basadas en documentos con soporte para filtros, distintos modos de respuesta y visualizaciÃ³n de resultados enriquecida.

## ğŸ“‚ Estructura del Proyecto

``` bash
project/
â”‚
â”œâ”€â”€ .env.template                  # API key de OpenAI
â”œâ”€â”€ .gitignore
â”œâ”€â”€ README.md
â”œâ”€â”€ requierements.txt
â”œâ”€â”€ renv.lock
â””â”€â”€ src
    â”‚
    â”œâ”€â”€ execute.py                 # CÃ³digo para ejecutar la creaciÃ³n del index y el retrieval segÃºn la pregunta
    â”œâ”€â”€ config/
    â”‚   â”‚
    â”‚   â”œâ”€â”€ paths.yaml             # Rutas a recursos estÃ¡ticos, estilos, logo, etc.
    â”‚   â””â”€â”€ llm.yaml               # ConfiguraciÃ³n del modelo LLM
    â”‚
    â”œâ”€â”€ utils/
    â”‚   â”‚
    â”‚   â”œâ”€â”€ config_functions.py    # Carga y resoluciÃ³n de rutas desde YAML
    â”‚   â””â”€â”€ llm_functions.py       # AbstracciÃ³n de modelos de embedding y respuesta
    â”‚
    â”œâ”€â”€ rag/                       # Archivos estÃ¡ticos (logo, otros)
    â”‚   â”‚
    â”‚   â”œâ”€â”€ create_index_functions # Script con las funciones para generar el index a partir de documentos
    â”‚   â””â”€â”€ query_index_functions  # Script con las funciones para hacer el retrieval a partir de una pregunta
    â”‚
    â””â”€â”€ persisted_index/           # Carpeta donde se guardarÃ¡ el index construido
```

## ğŸ§ª Requisitos

-   Python 3.9+
-   openai
-   python-dotenv
-   LlamaIndex

InstalaciÃ³n rÃ¡pida:

``` bash
pip install -r requirements.txt
```

Si usas [`renv`](https://rstudio.github.io/renv/articles/python.html) puedes ejecutar el siguiente comando para instalar las librerÃ­as python desde el `requirements.txt`:

``` bash
renv::restore()
```

## âš™ï¸ ConfiguraciÃ³n

1.  Claves de API

    - Cambia el nombre del archivo `.env.template` a `.env`
    - Si quieres usar los modelos de AZURE OpenAi, cambia a `True` el primer campo
    - Luego pega tu clave de AZURE OpenAi o de OpenAI

``` env
# Usa Azure en vez de OpenAI estandar
USE_AZURE_OPENAI = False

# Azure OpenAI
AZURE_OPENAI_API_KEY = '...'

# OpenAI (estandar)
OPENAI_API_KEY = 'sk-xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx'
```

2.  ConfiguraciÃ³n de rutas (`paths.yaml`)

    Contenido:

``` yaml
# Documents to index path
docs_to_index_path: 'C:/path/to/docs'

# Persisted Index path
persisted_index_path: '../persisted_index'
```

En el archivo anterior, solo debes ajustar `docs_to_index_path` de acuerdo a tu caso (esta ruta es la que define dÃ³nde estarÃ¡n los documentos que quieres indexar).

3.  ConfiguraciÃ³n del modelo (`llm.yaml`)

    Contenido:

``` yaml
# RAG response LLM and embedding LLM
rag:
  answer_question_llm:
    model_name: 'gpt-4o'
    azure_api_version: '...'
    azure_deployment_name: '...'
    azure_endpoint: '...'
  embedding_llm:
    model_name: 'text-embedding-3-large'
    azure_api_version: '...'
    azure_deployment_name: '...'
    azure_endpoint: '...'
```

Si quieres usar otros modelos de OpenAi, puedes cambiar el nombre del modelo modificando el valor de `model_name`.
Si quieres usar modelos OpenAi de AZURE, debes llenar los campos que estÃ¡n con `...`.

## ğŸ§  Â¿QuÃ© hace este proyecto?
1. IndexaciÃ³n Persistente
Lee documentos desde un directorio, aplica embeddings y construye un Ã­ndice vectorial que se guarda en disco:

``` bash
create_persisted_index(docs_dir  = "docs/",
                       index_dir = "index/",
                       index_id  = "mi_indice")
```

2. Consulta con RecuperaciÃ³n + GeneraciÃ³n (RAG)
Permite consultar el Ã­ndice con distintas estrategias de respuesta:

``` bash
query_persisted_index(index_dir         = "index/",
                      index_id          = "mi_indice",
                      query             = "Â¿QuÃ© dice el documento sobre X?",
                      response_mode     = "refine",
                      similarity_cutoff = 0.75)
```

Los modos de respuesta incluyen:

- refine: respuestas iterativas y refinadas.

- compact: respuestas directas y breves.

- simple_summarize: versiÃ³n simple del modo compacto.

- tree_summarize: fusiÃ³n jerÃ¡rquica de fragmentos.

- accumulate: acumulaciÃ³n de fragmentos.

- no_text: solo recuperaciÃ³n sin LLM.

## ğŸ“Œ CaracterÃ­sticas destacadas
Soporte para OpenAI y Azure OpenAI (conmutaciÃ³n automÃ¡tica vÃ­a .env)

- Filtros por metadatos y score (similarity_cutoff)

- Prompts personalizados por modo de respuesta

- Reporte de fragmentos utilizados por archivo

- Output enriquecido y trazabilidad del origen de la informaciÃ³n

## ğŸ“ Licencia

Este proyecto es de uso interno / acadÃ©mico / personal. ModifÃ­calo libremente segÃºn tus necesidades.